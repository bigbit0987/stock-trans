#!/usr/bin/env python
"""
RPS æ•°æ®æ›´æ–°ä»»åŠ¡ - é«˜æ€§èƒ½ç‰ˆæœ¬ v2.4.1
ç‰¹æ€§:
1. æ™ºèƒ½ç¼“å­˜: æ”¯æŒæ—¥å†…ç¼“å­˜å’Œå†å²æ•°æ®æŒä¹…åŒ–
2. å¢é‡æ›´æ–°: åªè·å–ç¼ºå¤±/è¿‡æœŸçš„æ•°æ®
3. æ‰¹é‡å¤„ç†: åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
4. å®æ—¶è¿›åº¦: æ˜¾ç¤ºé€Ÿåº¦å’Œé¢„ä¼°å‰©ä½™æ—¶é—´

v2.4.1 æ–°å¢:
- RPS20 çŸ­å‘¨æœŸæŒ‡æ ‡è®¡ç®—
- "å¼±è½¬å¼º"ä¿¡å·æ£€æµ‹ (å€Ÿé‰´ Qbot å¤šå‘¨æœŸ RPS ç­–ç•¥)
"""
import os
import sys
import datetime
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

import akshare as ak
from config.settings import STRATEGY, RPS_DATA_DIR, CONCURRENT, NETWORK, CACHE
from src.cache_manager import cache_manager
from src.utils import logger
from src.factors import get_market_condition
from src.data_loader import get_all_sector_mappings



def get_stock_momentum_fast(code: str, name: str, window: int = 120) -> Optional[Dict]:
    """
    å¿«é€Ÿè·å–è‚¡ç¥¨åŠ¨é‡æ•°æ® (v2.4.1 å¢å¼ºç‰ˆ)
    1. å…ˆæ£€æŸ¥åŠ¨é‡ç¼“å­˜
    2. å†æ£€æŸ¥å†å²æ•°æ®ç¼“å­˜
    3. æœ€åä»APIè·å–
    
    v2.4.1 æ–°å¢: åŒæ—¶è®¡ç®— RPS120 å’Œ RPS20 çš„åŠ¨é‡
    """
    # 1. æ£€æŸ¥åŠ¨é‡ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
    cached_momentum = cache_manager.get_momentum(code)
    if cached_momentum:
        return cached_momentum
    
    # 2. æ£€æŸ¥å†å²æ•°æ®ç¼“å­˜
    df = cache_manager.get_cached_history(code, window + 10)
    
    # 3. å¦‚æœç¼“å­˜æœªå‘½ä¸­æˆ–è¿‡æœŸï¼Œä»APIè·å–
    if df is None or len(df) < window:
        try:
            df = ak.stock_zh_a_hist(symbol=code, period="daily", adjust="qfq")
            if df is not None and len(df) > 0:
                # ä¿å­˜åˆ°å†å²ç¼“å­˜
                cache_manager.save_history_cache(code, df)
        except Exception as e:
            return None
    
    # 4. è®¡ç®—åŠ¨é‡
    if df is None or len(df) < window:
        return None
    
    try:
        close_now = df['æ”¶ç›˜'].iloc[-1]
        
        # v2.4.1: åŒæ—¶è®¡ç®—é•¿çŸ­å‘¨æœŸåŠ¨é‡
        close_prev_long = df['æ”¶ç›˜'].iloc[-window] if len(df) >= window else df['æ”¶ç›˜'].iloc[0]
        close_prev_short = df['æ”¶ç›˜'].iloc[-20] if len(df) >= 20 else df['æ”¶ç›˜'].iloc[0]
        
        pct_change_long = (close_now - close_prev_long) / close_prev_long
        pct_change_short = (close_now - close_prev_short) / close_prev_short
        
        # ä¿å­˜æœ€è¿‘4å¤©æ”¶ç›˜ä»·ï¼Œä¾›å®æ—¶è®¡ç®—MA5
        last_4_closes = df['æ”¶ç›˜'].tail(4).tolist()
        
        result = {
            'ä»£ç ': code,
            'åç§°': name,
            'momentum': pct_change_long,           # ä¸»åŠ¨é‡ (RPS120 æˆ–åŠ¨æ€å‘¨æœŸ)
            'momentum_short': pct_change_short,    # v2.4.1: çŸ­æœŸåŠ¨é‡ (RPS20)
            'æœ€æ–°ä»·': close_now,
            'MA5': df['æ”¶ç›˜'].tail(5).mean(),
            'last_4_closes_sum': sum(last_4_closes)
        }
        
        # ä¿å­˜åˆ°åŠ¨é‡ç¼“å­˜
        cache_manager.set_momentum(code, result)
        
        return result
    except Exception as e:
        return None


def run_updater():
    """æ‰§è¡Œ RPS æ•°æ®æ›´æ–°ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰"""
    logger.info("=" * 60)
    logger.info("ğŸ“Š RPS æ•°æ®æ›´æ–°å¯åŠ¨ (é«˜æ€§èƒ½ç‰ˆ)")
    logger.info(f"   æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"   å‘¨æœŸ: {STRATEGY.get('rps_window', 120)} å¤©")
    logger.info(f"   å¹¶å‘: {CONCURRENT.get('max_workers', 30)} çº¿ç¨‹")
    logger.info("=" * 60)
    
    # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
    stats = cache_manager.get_cache_stats()
    logger.info(f"\nğŸ“¦ ç¼“å­˜çŠ¶æ€:")
    logger.info(f"   å†å²æ•°æ®ç¼“å­˜: {stats['history_cached']} åª")
    logger.info(f"   åŠ¨é‡ç¼“å­˜: {stats['momentum_cached']} åª")
    logger.info(f"   ç¼“å­˜å¤§å°: {stats['cache_size_mb']} MB")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    logger.info("\nğŸ“¡ è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
    stock_info = ak.stock_zh_a_spot_em()
    stock_info = stock_info[['ä»£ç ', 'åç§°']]
    # è¿‡æ»¤æ‰ STã€é€€å¸‚å’Œæ–°è‚¡
    stock_info = stock_info[~stock_info['åç§°'].str.contains('ST|é€€|N')]
    
    total = len(stock_info)
    logger.info(f"   å…± {total} åªæ ‡çš„")
    
    # --- åŠ¨æ€ RPS å‘¨æœŸ (v2.4 æ–°å¢) ---
    default_window = STRATEGY.get('rps_window', 120)
    rps_window = default_window
    
    try:
        market_cond = get_market_condition()
        if market_cond.get('trend') == 'ä¸Šå‡è¶‹åŠ¿':
            rps_window = 20 # ç‰›å¸‚ç”¨æ›´æ•æ„Ÿçš„çŸ­æœŸåŠ¨é‡
            logger.info(f"   ğŸ“ˆ å¸‚åœºå¤„äºä¸Šå‡è¶‹åŠ¿ï¼Œä½¿ç”¨çŸ­æœŸ RPS å‘¨æœŸ: {rps_window}å¤©")
        elif market_cond.get('trend') == 'éœ‡è¡åå¼º':
            rps_window = 50
            logger.info(f"   ğŸ“Š å¸‚åœºéœ‡è¡åå¼ºï¼Œä½¿ç”¨ä¸­æœŸ RPS å‘¨æœŸ: {rps_window}å¤©")
        elif market_cond.get('trend') == 'ä¸‹é™è¶‹åŠ¿':
            rps_window = 120 # ç†Šå¸‚ç”¨é•¿æœŸåŠ¨é‡è¿‡æ»¤å™ªéŸ³
            logger.info(f"   ğŸ“‰ å¸‚åœºå¤„äºä¸‹é™è¶‹åŠ¿ï¼Œä½¿ç”¨é•¿æœŸ RPS å‘¨æœŸ: {rps_window}å¤©")
        else:
             logger.info(f"   âš–ï¸ ä½¿ç”¨é»˜è®¤ RPS å‘¨æœŸ: {rps_window}å¤©")
    except Exception as e:
        logger.warning(f"è·å–å¤§ç›˜çŠ¶æ€å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‘¨æœŸ: {e}")

    # åˆ†æ‰¹å¤„ç†
    batch_size = CONCURRENT.get('batch_size', 100)

    max_workers = CONCURRENT.get('max_workers', 30)
    all_results = []
    
    logger.info(f"\nğŸ”„ æ­£åœ¨è®¡ç®—ä¸ªè‚¡åŠ¨é‡ (çª—å£: {rps_window}å¤©)...")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}, å¹¶å‘çº¿ç¨‹: {max_workers}")
    
    start_time = time.time()
    
    for batch_idx in range(0, total, batch_size):
        batch_end = min(batch_idx + batch_size, total)
        batch_df = stock_info.iloc[batch_idx:batch_end]
        
        batch_results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(get_stock_momentum_fast, row['ä»£ç '], row['åç§°'], rps_window): row['ä»£ç ']
                for _, row in batch_df.iterrows()
            }
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    batch_results.append(result)
        
        all_results.extend(batch_results)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦
        elapsed = time.time() - start_time
        processed = len(all_results)
        rate = processed / elapsed if elapsed > 0 else 0
        remaining = (total - batch_end) / rate if rate > 0 else 0
        
        # æ¯5æ‰¹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (batch_idx // batch_size) % 5 == 0 or batch_end == total:
            logger.info(
                f"   [{batch_end}/{total}] "
                f"{batch_end*100//total}% | "
                f"é€Ÿåº¦: {rate:.1f}åª/ç§’ | "
                f"å‰©ä½™: {remaining:.0f}ç§’"
            )
    
    # ä¿å­˜åŠ¨é‡ç¼“å­˜
    cache_manager.save_momentum_cache()
    
    # è®¡ç®— RPS æ’å
    if not all_results:
        logger.error("\nâŒ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè®¡ç®—ç»ˆæ­¢")
        return
    
    rps_df = pd.DataFrame(all_results)
    
    # --- 1. å…¨å¸‚åœº RPS æ’å ---
    rps_df['RPS'] = rps_df['momentum'].rank(pct=True) * 100
    
    # --- 1.1 v2.4.1 æ–°å¢: RPS20 çŸ­å‘¨æœŸæ’å ---
    if 'momentum_short' in rps_df.columns:
        rps_df['RPS20'] = rps_df['momentum_short'].rank(pct=True) * 100
        logger.info("âœ… å·²è®¡ç®— RPS20 çŸ­å‘¨æœŸæŒ‡æ ‡")
    else:
        rps_df['RPS20'] = rps_df['RPS']  # å›é€€æ–¹æ¡ˆ
    
    # --- 1.2 v2.4.1 æ–°å¢: "å¼±è½¬å¼º"ä¿¡å·æ£€æµ‹ ---
    weak_to_strong_config = STRATEGY.get('rps_weak_to_strong', {})
    if weak_to_strong_config.get('enabled', True):
        rps120_threshold = weak_to_strong_config.get('rps120_threshold', 70)
        rps20_breakthrough = weak_to_strong_config.get('rps20_breakthrough', 90)
        
        # å¼±è½¬å¼º: RPS120 è¾ƒå¼±ä½† RPS20 çªç„¶èµ°å¼º
        rps_df['å¼±è½¬å¼º'] = (
            (rps_df['RPS'] < rps120_threshold) & 
            (rps_df['RPS20'] > rps20_breakthrough)
        )
        
        weak_to_strong_count = rps_df['å¼±è½¬å¼º'].sum()
        if weak_to_strong_count > 0:
            logger.info(f"ğŸš€ æ£€æµ‹åˆ° {weak_to_strong_count} åª'å¼±è½¬å¼º'ä¿¡å·è‚¡ (RPS120<{rps120_threshold} ä½† RPS20>{rps20_breakthrough})")
    else:
        rps_df['å¼±è½¬å¼º'] = False
    
    # --- 2. è¡Œä¸šæ¿å— RPS æ’å ---
    logger.info("\nğŸ“Š è®¡ç®—è¡Œä¸šæ¿å— RPS...")
    sector_map = get_all_sector_mappings()
    rps_df['æ¿å—'] = rps_df['ä»£ç '].map(sector_map).fillna('å…¶ä»–')
    
    # åˆ†ç»„è®¡ç®—æ¿å—å†… RPS
    rps_df['æ¿å—RPS'] = rps_df.groupby('æ¿å—')['momentum'].rank(pct=True) * 100
    # å¡«å……NaN (å¦‚æœæ¿å—åªæœ‰ä¸€ä¸ªè‚¡ç¥¨)
    rps_df['æ¿å—RPS'] = rps_df['æ¿å—RPS'].fillna(50) 
    
    # --- 3. RPS è¶‹åŠ¿è¿½è¸ª ---
    logger.info("ğŸ“ˆ è¿½è¸ª RPS å˜åŒ–è¶‹åŠ¿...")
    try:
        # å¯»æ‰¾æœ€è¿‘çš„ä¸€ä¸ªæ—§æ–‡ä»¶
        files = sorted([f for f in os.listdir(RPS_DATA_DIR) if f.startswith('rps_rank_')])
        if len(files) >= 1:
             # æ’é™¤ä»Šå¤©ç”Ÿæˆçš„æ–‡ä»¶(å¦‚æœå·²å­˜åœ¨)
             current_filename = f"rps_rank_{datetime.date.today().strftime('%Y%m%d')}.csv"
             history_files = [f for f in files if f != current_filename]
             
             if history_files:
                 last_file = history_files[-1]
                 prev_df = pd.read_csv(os.path.join(RPS_DATA_DIR, last_file))
                 # åˆ›å»º ä»£ç ->RPS æ˜ å°„
                 prev_rps_map = dict(zip(prev_df['ä»£ç '].astype(str).str.zfill(6), prev_df['RPS']))
                 
                 # è®¡ç®—å˜åŒ–
                 rps_df['RPSå˜åŠ¨'] = rps_df.apply(
                     lambda x: x['RPS'] - prev_rps_map.get(x['ä»£ç '], x['RPS']), axis=1
                 )
             else:
                 rps_df['RPSå˜åŠ¨'] = 0
        else:
             rps_df['RPSå˜åŠ¨'] = 0
             
    except Exception as e:
        logger.warning(f"RPS è¶‹åŠ¿è®¡ç®—å¤±è´¥: {e}")
        rps_df['RPSå˜åŠ¨'] = 0

    rps_df = rps_df.sort_values(by='RPS', ascending=False)

    
    # ä¿å­˜ç»“æœ
    today = datetime.date.today().strftime("%Y%m%d")
    os.makedirs(RPS_DATA_DIR, exist_ok=True)
    filepath = os.path.join(RPS_DATA_DIR, f"rps_rank_{today}.csv")
    rps_df.to_csv(filepath, index=False, encoding='utf-8-sig')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    
    logger.info(f"\nâœ… RPS æ•°æ®æ›´æ–°å®Œæˆ!")
    logger.info(f"   æ–‡ä»¶: {filepath}")
    logger.info(f"   æœ‰æ•ˆæ•°æ®: {len(rps_df)} åª")
    logger.info(f"   å¤„ç†é€Ÿåº¦: {len(all_results)/total_time:.1f} åª/ç§’")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    
    logger.info("\nğŸ“ˆ RPS å¼ºåº¦å‰ 15 å:")
    print_df = rps_df[['ä»£ç ', 'åç§°', 'RPS', 'RPS20', 'æ¿å—', 'æ¿å—RPS', 'RPSå˜åŠ¨', 'å¼±è½¬å¼º']].head(15)
    logger.info(print_df.to_string(index=False))
    
    # v2.4.1: æ˜¾ç¤ºå¼±è½¬å¼ºä¿¡å·è‚¡
    weak_to_strong_stocks = rps_df[rps_df['å¼±è½¬å¼º'] == True].head(10)
    if not weak_to_strong_stocks.empty:
        logger.info("\nğŸš€ å¼±è½¬å¼ºä¿¡å·è‚¡ (RPS120å¼±ä½†RPS20çªç„¶èµ°å¼º):")
        wts_df = weak_to_strong_stocks[['ä»£ç ', 'åç§°', 'RPS', 'RPS20', 'æ¿å—']]
        logger.info(wts_df.to_string(index=False))
    
    logger.info("=" * 60)
    
    return rps_df


if __name__ == "__main__":
    start = datetime.datetime.now()
    run_updater()
    duration = (datetime.datetime.now() - start).seconds
    logger.info(f"\nâ±ï¸ æ€»è€—æ—¶: {duration // 60} åˆ† {duration % 60} ç§’")
