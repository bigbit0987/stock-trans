#!/usr/bin/env python
"""
RPS æ•°æ®æ›´æ–°ä»»åŠ¡ - é«˜æ€§èƒ½ç‰ˆæœ¬
ç‰¹æ€§:
1. æ™ºèƒ½ç¼“å­˜: æ”¯æŒæ—¥å†…ç¼“å­˜å’Œå†å²æ•°æ®æŒä¹…åŒ–
2. å¢é‡æ›´æ–°: åªè·å–ç¼ºå¤±/è¿‡æœŸçš„æ•°æ®
3. æ‰¹é‡å¤„ç†: åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
4. å®æ—¶è¿›åº¦: æ˜¾ç¤ºé€Ÿåº¦å’Œé¢„ä¼°å‰©ä½™æ—¶é—´
"""
import os
import sys
import datetime
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

import akshare as ak
from config.settings import STRATEGY, RPS_DATA_DIR, CONCURRENT, NETWORK, CACHE
from src.cache_manager import cache_manager
from src.utils import logger


def get_stock_momentum_fast(code: str, name: str) -> Optional[Dict]:
    """
    å¿«é€Ÿè·å–è‚¡ç¥¨åŠ¨é‡æ•°æ®
    1. å…ˆæ£€æŸ¥åŠ¨é‡ç¼“å­˜
    2. å†æ£€æŸ¥å†å²æ•°æ®ç¼“å­˜
    3. æœ€åä»APIè·å–
    """
    window = STRATEGY.get('rps_window', 120)
    
    # 1. æ£€æŸ¥åŠ¨é‡ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
    cached_momentum = cache_manager.get_momentum(code)
    if cached_momentum:
        return cached_momentum
    
    # 2. æ£€æŸ¥å†å²æ•°æ®ç¼“å­˜
    df = cache_manager.get_cached_history(code, window + 10)
    
    # 3. å¦‚æœç¼“å­˜æœªå‘½ä¸­æˆ–è¿‡æœŸï¼Œä»APIè·å–
    if df is None or len(df) < window:
        try:
            df = ak.stock_zh_a_hist(symbol=code, period="daily", adjust="qfq")
            if df is not None and len(df) > 0:
                # ä¿å­˜åˆ°å†å²ç¼“å­˜
                cache_manager.save_history_cache(code, df)
        except Exception as e:
            return None
    
    # 4. è®¡ç®—åŠ¨é‡
    if df is None or len(df) < window:
        return None
    
    try:
        close_now = df['æ”¶ç›˜'].iloc[-1]
        close_prev = df['æ”¶ç›˜'].iloc[-window]
        pct_change = (close_now - close_prev) / close_prev
        
        # ä¿å­˜æœ€è¿‘4å¤©æ”¶ç›˜ä»·ï¼Œä¾›å®æ—¶è®¡ç®—MA5
        last_4_closes = df['æ”¶ç›˜'].tail(4).tolist()
        
        result = {
            'ä»£ç ': code,
            'åç§°': name,
            'momentum': pct_change,
            'æœ€æ–°ä»·': close_now,
            'MA5': df['æ”¶ç›˜'].tail(5).mean(),
            'last_4_closes_sum': sum(last_4_closes)
        }
        
        # ä¿å­˜åˆ°åŠ¨é‡ç¼“å­˜
        cache_manager.set_momentum(code, result)
        
        return result
    except Exception as e:
        return None


def run_updater():
    """æ‰§è¡Œ RPS æ•°æ®æ›´æ–°ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰"""
    logger.info("=" * 60)
    logger.info("ğŸ“Š RPS æ•°æ®æ›´æ–°å¯åŠ¨ (é«˜æ€§èƒ½ç‰ˆ)")
    logger.info(f"   æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"   å‘¨æœŸ: {STRATEGY.get('rps_window', 120)} å¤©")
    logger.info(f"   å¹¶å‘: {CONCURRENT.get('max_workers', 30)} çº¿ç¨‹")
    logger.info("=" * 60)
    
    # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
    stats = cache_manager.get_cache_stats()
    logger.info(f"\nğŸ“¦ ç¼“å­˜çŠ¶æ€:")
    logger.info(f"   å†å²æ•°æ®ç¼“å­˜: {stats['history_cached']} åª")
    logger.info(f"   åŠ¨é‡ç¼“å­˜: {stats['momentum_cached']} åª")
    logger.info(f"   ç¼“å­˜å¤§å°: {stats['cache_size_mb']} MB")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    logger.info("\nğŸ“¡ è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
    stock_info = ak.stock_zh_a_spot_em()
    stock_info = stock_info[['ä»£ç ', 'åç§°']]
    # è¿‡æ»¤æ‰ STã€é€€å¸‚å’Œæ–°è‚¡
    stock_info = stock_info[~stock_info['åç§°'].str.contains('ST|é€€|N')]
    
    total = len(stock_info)
    logger.info(f"   å…± {total} åªæ ‡çš„")
    
    # åˆ†æ‰¹å¤„ç†
    batch_size = CONCURRENT.get('batch_size', 100)
    max_workers = CONCURRENT.get('max_workers', 30)
    all_results = []
    
    logger.info(f"\nğŸ”„ æ­£åœ¨è®¡ç®—ä¸ªè‚¡åŠ¨é‡...")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}, å¹¶å‘çº¿ç¨‹: {max_workers}")
    
    start_time = time.time()
    cache_hits = 0
    api_calls = 0
    
    for batch_idx in range(0, total, batch_size):
        batch_end = min(batch_idx + batch_size, total)
        batch_df = stock_info.iloc[batch_idx:batch_end]
        
        batch_results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(get_stock_momentum_fast, row['ä»£ç '], row['åç§°']): row['ä»£ç ']
                for _, row in batch_df.iterrows()
            }
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    batch_results.append(result)
        
        all_results.extend(batch_results)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦
        elapsed = time.time() - start_time
        processed = len(all_results)
        rate = processed / elapsed if elapsed > 0 else 0
        remaining = (total - batch_end) / rate if rate > 0 else 0
        
        # æ¯5æ‰¹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (batch_idx // batch_size) % 5 == 0 or batch_end == total:
            logger.info(
                f"   [{batch_end}/{total}] "
                f"{batch_end*100//total}% | "
                f"é€Ÿåº¦: {rate:.1f}åª/ç§’ | "
                f"å‰©ä½™: {remaining:.0f}ç§’"
            )
    
    # ä¿å­˜åŠ¨é‡ç¼“å­˜
    cache_manager.save_momentum_cache()
    
    # è®¡ç®— RPS æ’å
    if not all_results:
        logger.error("\nâŒ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè®¡ç®—ç»ˆæ­¢")
        return
    
    rps_df = pd.DataFrame(all_results)
    # è®¡ç®—ç™¾åˆ†æ¯”æ’å
    rps_df['RPS'] = rps_df['momentum'].rank(pct=True) * 100
    rps_df = rps_df.sort_values(by='RPS', ascending=False)
    
    # ä¿å­˜ç»“æœ
    today = datetime.date.today().strftime("%Y%m%d")
    os.makedirs(RPS_DATA_DIR, exist_ok=True)
    filepath = os.path.join(RPS_DATA_DIR, f"rps_rank_{today}.csv")
    rps_df.to_csv(filepath, index=False, encoding='utf-8-sig')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    
    logger.info(f"\nâœ… RPS æ•°æ®æ›´æ–°å®Œæˆ!")
    logger.info(f"   æ–‡ä»¶: {filepath}")
    logger.info(f"   æœ‰æ•ˆæ•°æ®: {len(rps_df)} åª")
    logger.info(f"   å¤„ç†é€Ÿåº¦: {len(all_results)/total_time:.1f} åª/ç§’")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    
    logger.info("\nğŸ“ˆ RPS å¼ºåº¦å‰ 15 å:")
    print_df = rps_df[['ä»£ç ', 'åç§°', 'RPS', 'momentum']].head(15)
    logger.info(print_df.to_string(index=False))
    logger.info("=" * 60)
    
    return rps_df


if __name__ == "__main__":
    start = datetime.datetime.now()
    run_updater()
    duration = (datetime.datetime.now() - start).seconds
    logger.info(f"\nâ±ï¸ æ€»è€—æ—¶: {duration // 60} åˆ† {duration % 60} ç§’")
