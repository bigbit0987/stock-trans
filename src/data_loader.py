#!/usr/bin/env python
"""
æ•°æ®è·å–æ¨¡å— (v2.4 å¢å¼ºç‰ˆ)
åŠŸèƒ½:
1. æ™ºèƒ½ç¼“å­˜ + æ‰¹é‡è·å–
2. tenacity æŒ‡æ•°é€€é¿é‡è¯•ï¼ˆç½‘ç»œé²æ£’æ€§ï¼‰
3. æ—¥æœŸæ ¡éªŒï¼ˆé˜²æ­¢ MA5 ç­‰æŒ‡æ ‡çš„æœªæ¥å‡½æ•°é”™è¯¯ï¼‰
"""
import akshare as ak
import pandas as pd
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Dict, Callable

# tenacity é‡è¯•åº“
try:
    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
    HAS_TENACITY = True
except ImportError:
    HAS_TENACITY = False

# å¯¼å…¥é…ç½®
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import STRATEGY, RPS_DATA_DIR, CONCURRENT, NETWORK, CACHE
from src.cache_manager import cache_manager
from src.utils import logger, ensure_history_excludes_today

# ============================================
# æ•°æ®æºæ ‡å‡†æ˜ å°„ (v2.5.0: è§£å†³ Akshare å­—æ®µå˜åŠ¨é—®é¢˜)
# ============================================

# å®æ—¶è¡Œæƒ…å­—æ®µæ˜ å°„
REALTIME_COL_MAP = {
    'ä»£ç ': 'code',
    'åç§°': 'name',
    'æœ€æ–°ä»·': 'close',
    'ä»Šå¼€': 'open',
    'æœ€é«˜': 'high',
    'æœ€ä½': 'low',
    'æˆäº¤é‡': 'volume',
    'æˆäº¤é¢': 'amount',
    'æ¶¨è·Œå¹…': 'pct_change',
    'æ¢æ‰‹ç‡': 'turnover',
    'é‡æ¯”': 'volume_ratio',
    'å¸‚ç›ˆç‡-åŠ¨æ€': 'pe',
    'å¸‚å‡€ç‡': 'pb',
    'æ€»å¸‚å€¼': 'market_cap',
}

# å†å²è¡Œæƒ…å­—æ®µæ˜ å°„
HIST_COL_MAP = {
    'æ—¥æœŸ': 'date',
    'å¼€ç›˜': 'open',
    'æ”¶ç›˜': 'close',
    'æœ€é«˜': 'high',
    'æœ€ä½': 'low',
    'æˆäº¤é‡': 'volume',
    'æˆäº¤é¢': 'amount',
    'æŒ¯å¹…': 'amplitude',
    'æ¶¨è·Œå¹…': 'pct_change',
    'æ¢æ‰‹ç‡': 'turnover',
}

def standardize_df(df: pd.DataFrame, col_map: Dict[str, str]) -> pd.DataFrame:
    """
    ç»Ÿä¸€ DataFrame åˆ—åï¼Œå¢å¼ºç³»ç»ŸæŠ—æ³¢åŠ¨èƒ½åŠ›
    """
    if df is None or df.empty:
        return df
    return df.rename(columns=col_map)


# ============================================
# æ™ºèƒ½é‡è¯•è£…é¥°å™¨ (v2.4 tenacity å¢å¼ºç‰ˆ)
# ============================================

def retry_on_failure(max_retries: int = 3, delay: float = 0.5):
    """
    æ™ºèƒ½é‡è¯•è£…é¥°å™¨
    
    v2.4 å¢å¼º:
    - ä½¿ç”¨ tenacity å®ç°æ›´ä¸“ä¸šçš„æŒ‡æ•°é€€é¿
    - è‡ªåŠ¨è¯†åˆ«å¯é‡è¯•çš„å¼‚å¸¸ç±»å‹
    - è¶…æ—¶ä¿æŠ¤
    """
    def decorator(func):
        if HAS_TENACITY:
            # ä½¿ç”¨ tenacity çš„æŒ‡æ•°é€€é¿é‡è¯•
            @retry(
                stop=stop_after_attempt(max_retries),
                wait=wait_exponential(multiplier=delay, min=0.5, max=10),
                retry=retry_if_exception_type((ConnectionError, TimeoutError, OSError)),
                reraise=True
            )
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
            return wrapper
        else:
            # é™çº§ä½¿ç”¨ç®€å•é‡è¯•
            def wrapper(*args, **kwargs):
                last_exception = None
                for attempt in range(max_retries):
                    try:
                        return func(*args, **kwargs)
                    except Exception as e:
                        last_exception = e
                        if attempt < max_retries - 1:
                            time.sleep(delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                return None
            return wrapper
    return decorator


def get_all_stocks() -> pd.DataFrame:
    """è·å–å…¨å¸‚åœº A è‚¡åˆ—è¡¨"""
    logger.info("ğŸ“¡ è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
    df = ak.stock_zh_a_spot_em()
    
    # è¿‡æ»¤ STã€é€€å¸‚ã€æ–°è‚¡
    df = df[~df['åç§°'].str.contains('ST|é€€|N')]
    
    logger.info(f"   å…± {len(df)} åªè‚¡ç¥¨")
    return df


def get_realtime_quotes() -> pd.DataFrame:
    """è·å–å®æ—¶è¡Œæƒ…æ•°æ®å¹¶æ ‡å‡†åŒ– (v2.5.0)"""
    logger.info("ğŸ“¡ è·å–å®æ—¶è¡Œæƒ…...")
    df = ak.stock_zh_a_spot_em()
    
    # æ ‡å‡†åŒ–åˆ—å
    df = standardize_df(df, REALTIME_COL_MAP)
    
    # ---ã€v2.5.0 å…¼å®¹æ€§å¢å¼ºï¼šä¿ç•™ä¸­æ–‡ç´¢å¼•å‰¯æœ¬ã€‘---
    # è¿™æ ·æ—¢èƒ½è®©æ—§ä»£ç è·‘é€šï¼Œåˆèƒ½è®©æ–°é€»è¾‘ä½¿ç”¨è‹±æ–‡æ ‡å‡†åˆ—
    compat_map = {v: k for k, v in REALTIME_COL_MAP.items()}
    for eng, chn in compat_map.items():
        if eng in df.columns:
            df[chn] = df[eng]
    
    # è¡¥å……è®¡ç®—å­—æ®µçš„æ ‡å‡†åŒ–æ˜ å°„
    if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:
        df['amplitude'] = (df['high'] - df['low']) / df['close'].shift(1).fillna(df['open'])
        df['æŒ¯å¹…'] = df['amplitude']
        df['is_up'] = df['close'] > df['open']
        df['æ˜¯é˜³çº¿'] = df['is_up']
    
    logger.info(f"   è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
    return df


@retry_on_failure(max_retries=NETWORK.get('max_retries', 3), delay=NETWORK.get('retry_delay', 0.5))
def get_tail_volume_ratio(code: str) -> float:
    """
    è®¡ç®—å°¾ç›˜ 15 åˆ†é’Ÿæˆäº¤é‡å æ¯” (v2.5.0)
    
    é€»è¾‘ï¼š
    1. è·å–å½“æ—¥ 1 åˆ†é’Ÿæ•°æ®
    2. è®¡ç®— 14:45 - 15:00 çš„æˆäº¤é‡æ€»å’Œ
    3. è®¡ç®—å…¨å¤©æˆäº¤é‡æ€»å’Œ
    4. è¿”å›æ¯”ä¾‹
    """
    try:
        df = ak.stock_zh_a_hist_min_em(symbol=code, period='1', adjust='qfq')
        if df is None or df.empty:
            return 0.0
        
        # ç¡®ä¿æ—¶é—´æ˜¯å­—ç¬¦ä¸²å¹¶è¿‡æ»¤å½“æ—¥æ•°æ®
        today_str = datetime.datetime.now().strftime('%Y-%m-%d')
        # å¦‚æœæ˜¯å¤œé—´æµ‹è¯•æˆ–éäº¤æ˜“æ—¥ï¼Œå–æœ€åä¸€å¤©
        last_date = df.iloc[-1]['æ—¶é—´'].split(' ')[0]
        df_today = df[df['æ—¶é—´'].str.startswith(last_date)]
        
        if df_today.empty:
            return 0.0
            
        total_volume = df_today['æˆäº¤é‡'].sum()
        # å–æœ€å 15 æ ¹ K çº¿
        tail_df = df_today.tail(15)
        tail_volume = tail_df['æˆäº¤é‡'].sum()
        
        if total_volume > 0:
            return round(tail_volume / total_volume * 100, 2)
        return 0.0
    except Exception as e:
        logger.debug(f"è·å– {code} å°¾ç›˜æ•°æ®å¤±è´¥: {e}")
        return 0.0


@retry_on_failure(max_retries=NETWORK.get('max_retries', 3), delay=NETWORK.get('retry_delay', 0.5))
def _fetch_stock_history_from_api(code: str, days: int = 150, adjust: str = "qfq") -> Optional[pd.DataFrame]:
    """
    ä»APIè·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
    
    v2.4 å¢å¼º: ä½¿ç”¨ tenacity æŒ‡æ•°é€€é¿é‡è¯•
    """
    try:
        df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date="20200101", adjust=adjust)
        if df is None or df.empty:
            return None
        
        # æ ‡å‡†åŒ–åˆ—å (v2.5.0)
        df = standardize_df(df, HIST_COL_MAP)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            
        return df.tail(days + 10)
    except Exception as e:
        logger.error(f"è·å– {code} å†å²æ•°æ® API å¤±è´¥: {e}")
        return None


def get_stock_history(
    code: str, 
    days: int = 30, 
    adjust: str = "qfq", 
    use_cache: bool = True,
    exclude_today: bool = True  # v2.4 æ–°å¢: æ˜¯å¦æ’é™¤ä»Šæ—¥æ•°æ®
) -> Optional[pd.DataFrame]:
    """
    è·å–å•åªè‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ + æ—¥æœŸæ ¡éªŒï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        days: è·å–å¤©æ•°
        adjust: å¤æƒç±»å‹ (qfq=å‰å¤æƒ, hfq=åå¤æƒ, ""=ä¸å¤æƒ)
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        exclude_today: æ˜¯å¦æ’é™¤ä»Šæ—¥æ•°æ®ï¼ˆé˜²æ­¢ MA5 ç­‰æŒ‡æ ‡è®¡ç®—é”™è¯¯ï¼‰
    
    v2.4 å¢å¼º:
    - è‡ªåŠ¨æ’é™¤ä»Šæ—¥æ•°æ®ï¼Œé¿å… MA5 è®¡ç®—æ—¶çš„"æœªæ¥å‡½æ•°"é”™è¯¯
    - ä½¿ç”¨ tenacity å¢å¼ºç½‘ç»œé‡è¯•
    """
    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if use_cache and CACHE.get('enabled', True):
        cached = cache_manager.get_cached_history(code, days)
        if cached is not None and len(cached) >= days:
            df = cached.tail(days + 10)
            # v2.4: æ—¥æœŸæ ¡éªŒï¼Œæ’é™¤ä»Šæ—¥
            if exclude_today:
                df = ensure_history_excludes_today(df)
            return df
    
    # 2. ä»APIè·å–
    try:
        df = _fetch_stock_history_from_api(code, days, adjust)
        
        if df is not None:
            # v2.4: æ—¥æœŸæ ¡éªŒï¼Œæ’é™¤ä»Šæ—¥
            if exclude_today:
                df = ensure_history_excludes_today(df)
            
            if use_cache:
                # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆä¿å­˜åŸå§‹æ•°æ®ï¼Œä¸å«ä»Šæ—¥æ ¡éªŒï¼‰
                cache_manager.save_history_cache(code, df)
        
        return df
    except Exception as e:
        logger.debug(f"è·å– {code} å†å²æ•°æ®å¤±è´¥: {e}")
        return None


def get_stock_history_range(
    code: str, 
    start_date: str, 
    end_date: str, 
    adjust: str = "qfq"
) -> Optional[pd.DataFrame]:
    """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å†å²æ•°æ®"""
    try:
        df = ak.stock_zh_a_hist(
            symbol=code, 
            period="daily", 
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )
        return df if len(df) > 0 else None
    except Exception:
        return None


def batch_get_history(
    codes: List[str], 
    days: int = 30,
    progress_callback: Callable = None,
    use_cache: bool = True
) -> Dict[str, pd.DataFrame]:
    """
    æ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆå¤šçº¿ç¨‹ + ç¼“å­˜ï¼‰
    """
    results = {}
    processed = 0
    total = len(codes)
    cache_hits = 0
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    codes_to_fetch = []
    if use_cache and CACHE.get('enabled', True):
        for code in codes:
            cached = cache_manager.get_cached_history(code, days)
            if cached is not None and len(cached) >= days:
                results[code] = cached.tail(days + 10)
                cache_hits += 1
            else:
                codes_to_fetch.append(code)
    else:
        codes_to_fetch = codes
    
    if cache_hits > 0:
        logger.info(f"   ğŸ“¦ ç¼“å­˜å‘½ä¸­: {cache_hits}/{total}")
    
    # ä»APIè·å–å‰©ä½™æ•°æ®
    if codes_to_fetch:
        max_workers = CONCURRENT.get('max_workers', 30)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(get_stock_history, code, days, "qfq", use_cache): code 
                for code in codes_to_fetch
            }
            
            for future in as_completed(futures):
                code = futures[future]
                processed += 1
                
                try:
                    df = future.result()
                    if df is not None:
                        results[code] = df
                except Exception as e:
                    pass
                
                if progress_callback and processed % 100 == 0:
                    progress_callback(processed + cache_hits, total)
    
    return results


def load_latest_rps() -> Optional[pd.DataFrame]:
    """åŠ è½½æœ€æ–°çš„ RPS æ•°æ®"""
    if not os.path.exists(RPS_DATA_DIR):
        return None
    
    files = sorted([f for f in os.listdir(RPS_DATA_DIR) if f.startswith('rps_rank_')])
    if not files:
        return None
    
    latest_file = files[-1]
    filepath = os.path.join(RPS_DATA_DIR, latest_file)
    
    logger.info(f"ğŸ“– åŠ è½½ RPS æ•°æ®: {latest_file}")
    df = pd.read_csv(filepath)
    df['ä»£ç '] = df['ä»£ç '].astype(str).str.zfill(6)
    
    return df


def get_cached_momentum(code: str) -> Optional[dict]:
    """è·å–ç¼“å­˜çš„åŠ¨é‡æ•°æ®ï¼ˆä¾›scannerä½¿ç”¨ï¼‰"""
    return cache_manager.get_momentum(code)


def get_cache_stats() -> dict:
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    return cache_manager.get_cache_stats()


def get_all_sector_mappings(use_cache: bool = True) -> Dict[str, str]:
    """
    è·å–å…¨å¸‚åœºè‚¡ç¥¨çš„æ¿å—æ˜ å°„ (Code -> SectorName)
    
    ç­–ç•¥:
    1. ä¼˜å…ˆè¯»å–æœ¬åœ°ç¼“å­˜ (data/sector_map.json)
    2. å¦‚æœç¼“å­˜è¿‡æœŸ(>7å¤©)æˆ–å¼ºåˆ¶åˆ·æ–°, åˆ™ä»APIè·å–
    3. APIè·å–æ–¹å¼: è·å–æ‰€æœ‰æ¿å—åç§° -> å¹¶å‘è·å–æ¯ä¸ªæ¿å—çš„æˆåˆ†è‚¡ -> æ„å»ºæ˜ å°„
    """
    import json
    
    SECTOR_MAP_FILE = os.path.join(RPS_DATA_DIR, "sector_map.json")
    
    # 1. å°è¯•è¯»å–ç¼“å­˜
    if use_cache and os.path.exists(SECTOR_MAP_FILE):
        try:
            # æ£€æŸ¥æ–‡ä»¶æ—¶é—´
            mtime = os.path.getmtime(SECTOR_MAP_FILE)
            if time.time() - mtime < 7 * 24 * 3600: # 7å¤©æœ‰æ•ˆæœŸ
                with open(SECTOR_MAP_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except (json.JSONDecodeError, IOError, OSError):
            pass
            
    logger.info("ğŸ“¡ æ­£åœ¨å…¨é‡æ›´æ–°æ¿å—æ•°æ® (å¤§æ¦‚éœ€è¦ 1-2 åˆ†é’Ÿ)...")
    
    mapping = {}
    try:
        # è·å–æ‰€æœ‰è¡Œä¸šæ¿å—
        boards = ak.stock_board_industry_name_em()
        if boards is None or boards.empty:
            return {}
            
        board_names = boards['æ¿å—åç§°'].tolist()
        
        # å¹¶å‘è·å–æ¿å—æˆåˆ†è‚¡
        def _get_cons(name):
            try:
                df = ak.stock_board_industry_cons_em(symbol=name)
                if df is not None and not df.empty:
                    return name, df['ä»£ç '].tolist()
            except Exception:
                return name, []
            return name, []

        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_board = {executor.submit(_get_cons, name): name for name in board_names}
            
            processed = 0
            for future in as_completed(future_to_board):
                name, codes = future.result()
                for code in codes:
                    # ä¸€ä¸ªè‚¡ç¥¨å¯èƒ½å±äºå¤šä¸ªæ¿å—å—ï¼Ÿä¸œæ–¹è´¢å¯Œçš„è¡Œä¸šæ¿å—é€šå¸¸æ˜¯ä¸»è¡Œä¸š
                    # è¿™é‡Œç®€å•çš„è¦†ç›–ï¼Œæˆ–è€…ä¿ç•™ç¬¬ä¸€ä¸ª
                    if code not in mapping:
                        mapping[code] = name
                
                processed += 1
                if processed % 10 == 0:
                    print(f"\r   è¿›åº¦: {processed}/{len(board_names)}", end="")
        
        print("") # new line
        
        # ä¿å­˜ç¼“å­˜
        os.makedirs(os.path.dirname(SECTOR_MAP_FILE), exist_ok=True)
        with open(SECTOR_MAP_FILE, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False)
            
        logger.info(f"   âœ… æ¿å—æ•°æ®æ›´æ–°å®Œæˆï¼Œå…± {len(mapping)} åªè‚¡ç¥¨å½’ç±»")
        return mapping
        
    except Exception as e:
        logger.error(f"   âŒ è·å–æ¿å—æ•°æ®å¤±è´¥: {e}")
        # å¦‚æœå¤±è´¥ä¸”æœ‰æ—§ç¼“å­˜ï¼Œå°è¯•è¯»å–æ—§ç¼“å­˜
        if os.path.exists(SECTOR_MAP_FILE):
            try:
                with open(SECTOR_MAP_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError):
                pass
        return {}
