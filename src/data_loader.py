#!/usr/bin/env python
"""
æ•°æ®è·å–æ¨¡å— - ä¼˜åŒ–ç‰ˆ
æ”¯æŒç¼“å­˜ã€é‡è¯•ã€æ‰¹é‡è·å–
"""
import akshare as ak
import pandas as pd
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Dict, Callable

# å¯¼å…¥é…ç½®
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import STRATEGY, RPS_DATA_DIR, CONCURRENT, NETWORK, CACHE
from src.cache_manager import cache_manager
from src.utils import logger


def retry_on_failure(max_retries: int = 3, delay: float = 0.5):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        time.sleep(delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
            return None
        return wrapper
    return decorator


def get_all_stocks() -> pd.DataFrame:
    """è·å–å…¨å¸‚åœº A è‚¡åˆ—è¡¨"""
    logger.info("ğŸ“¡ è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
    df = ak.stock_zh_a_spot_em()
    
    # è¿‡æ»¤ STã€é€€å¸‚ã€æ–°è‚¡
    df = df[~df['åç§°'].str.contains('ST|é€€|N')]
    
    logger.info(f"   å…± {len(df)} åªè‚¡ç¥¨")
    return df


def get_realtime_quotes() -> pd.DataFrame:
    """è·å–å®æ—¶è¡Œæƒ…æ•°æ®"""
    logger.info("ğŸ“¡ è·å–å®æ—¶è¡Œæƒ…...")
    df = ak.stock_zh_a_spot_em()
    
    # è®¡ç®—æŒ¯å¹…
    df['æŒ¯å¹…'] = (df['æœ€é«˜'] - df['æœ€ä½']) / df['æ˜¨æ”¶'] 
    
    # åˆ¤æ–­é˜³çº¿
    df['æ˜¯é˜³çº¿'] = df['æœ€æ–°ä»·'] > df['ä»Šå¼€']
    
    logger.info(f"   è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
    return df


@retry_on_failure(max_retries=NETWORK.get('max_retries', 3), delay=NETWORK.get('retry_delay', 0.5))
def _fetch_stock_history_from_api(code: str, days: int = 150, adjust: str = "qfq") -> Optional[pd.DataFrame]:
    """ä»APIè·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
    df = ak.stock_zh_a_hist(symbol=code, period="daily", adjust=adjust)
    if df is None or len(df) < days:
        return None
    return df.tail(days + 10)


def get_stock_history(code: str, days: int = 30, adjust: str = "qfq", use_cache: bool = True) -> Optional[pd.DataFrame]:
    """
    è·å–å•åªè‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        days: è·å–å¤©æ•°
        adjust: å¤æƒç±»å‹ (qfq=å‰å¤æƒ, hfq=åå¤æƒ, ""=ä¸å¤æƒ)
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    """
    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if use_cache and CACHE.get('enabled', True):
        cached = cache_manager.get_cached_history(code, days)
        if cached is not None and len(cached) >= days:
            return cached.tail(days + 10)
    
    # 2. ä»APIè·å–
    try:
        df = _fetch_stock_history_from_api(code, days, adjust)
        
        if df is not None and use_cache:
            # ä¿å­˜åˆ°ç¼“å­˜
            cache_manager.save_history_cache(code, df)
        
        return df
    except Exception as e:
        return None


def get_stock_history_range(
    code: str, 
    start_date: str, 
    end_date: str, 
    adjust: str = "qfq"
) -> Optional[pd.DataFrame]:
    """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å†å²æ•°æ®"""
    try:
        df = ak.stock_zh_a_hist(
            symbol=code, 
            period="daily", 
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )
        return df if len(df) > 0 else None
    except:
        return None


def batch_get_history(
    codes: List[str], 
    days: int = 30,
    progress_callback: Callable = None,
    use_cache: bool = True
) -> Dict[str, pd.DataFrame]:
    """
    æ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆå¤šçº¿ç¨‹ + ç¼“å­˜ï¼‰
    """
    results = {}
    processed = 0
    total = len(codes)
    cache_hits = 0
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    codes_to_fetch = []
    if use_cache and CACHE.get('enabled', True):
        for code in codes:
            cached = cache_manager.get_cached_history(code, days)
            if cached is not None and len(cached) >= days:
                results[code] = cached.tail(days + 10)
                cache_hits += 1
            else:
                codes_to_fetch.append(code)
    else:
        codes_to_fetch = codes
    
    if cache_hits > 0:
        logger.info(f"   ğŸ“¦ ç¼“å­˜å‘½ä¸­: {cache_hits}/{total}")
    
    # ä»APIè·å–å‰©ä½™æ•°æ®
    if codes_to_fetch:
        max_workers = CONCURRENT.get('max_workers', 30)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(get_stock_history, code, days, "qfq", use_cache): code 
                for code in codes_to_fetch
            }
            
            for future in as_completed(futures):
                code = futures[future]
                processed += 1
                
                try:
                    df = future.result()
                    if df is not None:
                        results[code] = df
                except Exception as e:
                    pass
                
                if progress_callback and processed % 100 == 0:
                    progress_callback(processed + cache_hits, total)
    
    return results


def load_latest_rps() -> Optional[pd.DataFrame]:
    """åŠ è½½æœ€æ–°çš„ RPS æ•°æ®"""
    if not os.path.exists(RPS_DATA_DIR):
        return None
    
    files = sorted([f for f in os.listdir(RPS_DATA_DIR) if f.startswith('rps_rank_')])
    if not files:
        return None
    
    latest_file = files[-1]
    filepath = os.path.join(RPS_DATA_DIR, latest_file)
    
    logger.info(f"ğŸ“– åŠ è½½ RPS æ•°æ®: {latest_file}")
    df = pd.read_csv(filepath)
    df['ä»£ç '] = df['ä»£ç '].astype(str).str.zfill(6)
    
    return df


def get_cached_momentum(code: str) -> Optional[dict]:
    """è·å–ç¼“å­˜çš„åŠ¨é‡æ•°æ®ï¼ˆä¾›scannerä½¿ç”¨ï¼‰"""
    return cache_manager.get_momentum(code)


def get_cache_stats() -> dict:
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    return cache_manager.get_cache_stats()
